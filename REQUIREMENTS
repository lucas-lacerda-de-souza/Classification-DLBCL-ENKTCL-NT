System Requirements
• Operating System
Ubuntu 20.04.1 LTS (64-bit)
Fully compatible with CUDA, PyTorch, multi-GPU training, and large-scale histopathology workflows.

• Programming Language
Python 3.12.11

• Frameworks and Libraries
Core Deep Learning Stack
PyTorch 2.8.0 (CUDA 12.8)
TorchVision 0.19.0
TorchAudio 2.8.0

Data Handling and Processing
NumPy 1.26.4
Pandas 2.2.3
OpenPyXL 3.1.5 (for clinical Excel metadata)

Visualization
Matplotlib 3.9.2
Seaborn 0.13.2

Image Processing
Pillow 10.4.0

Utilities
tqdm 4.66.5 (progress bars)

Model Interpretation
SHAP (installed automatically with scikit-learn-compatible version)
Grad-CAM (implemented inside this repository)

• Conda / Channel Requirements
This project was developed with conda and uses:
pytorch
nvidia
defaults
These channels ensure access to the correct CUDA-enabled builds of PyTorch and TorchVision for multi-GPU training.

• Hardware Requirements
CPU
Intel Xeon W-2295
18 cores / 36 threads — recommended for parallel patch preprocessing, morphometry calculations, and Grad-CAM generation.

RAM
125 GB
Required for WSI handling, U-Net++ segmentation, and multi-model training pipelines.

GPU
3 × NVIDIA GeForce RTX 3090 (24 GB each)
Fully compatible with:
Multi-GPU DataParallel training
U-Net++ segmentation
ConvNeXt-XLarge training
Large-batch Grad-CAM computation

Disk Space
Minimum 1 TB recommended
Whole-slide images
Patch datasets (~299×299)
Segmentation masks
Logs and checkpoints
Multimodal model weights

• Software Requirements
Core Components
Deep Learning Framework: PyTorch
Segmentation Framework: U-Net++ with attention (implemented within repository)
Classification Models:
AlexNet
ResNet50
ConvNeXt-XLarge
Segmentation-enhanced variants
Multimodal Fusion Module: PyTorch + Scikit-learn
Morphometric Extraction: Python + NumPy + SciPy + custom geometry functions

• Model Interpretation:
SHAP for structured variables
Grad-CAM for CNN explainability
Traditional Machine Learning:
XGBoost (via scikit-learn API)

• Data and Code Availability
Data
Due to ethical restrictions and patient confidentiality, WSI datasets cannot be publicly released.
Synthetic demo data and example patches are provided in the repository for running inference and testing the pipeline.

• Code
The full multimodal and segmentation framework is available on GitHub: https://github.com/lucas-lacerda-de-souza/Classification-DLBCL-ENKTCL-NT

• Model Weights
Pretrained models, segmentation weights, and multimodal checkpoints are deposited in Zenodo: https://doi.org/10.5281/zenodo.17661989


